#!/usr/bin/env python

import sys

from requests import get
from requests.exceptions import HTTPError, RequestException

TIMEOUT = 30
MAX_TRIES = 3


def archive_web_page(url, tries=0):
    api_url = f"http://web.archive.org/save/{url}"
    available_api_url = f"http://archive.org/wayback/available?url={url}"

    try:
        # Attempt to archive the page
        response = get(api_url, timeout=TIMEOUT)
        response.raise_for_status()

        # Check if the page was archived
        response = get(available_api_url, timeout=TIMEOUT)
        response.raise_for_status()

        if response.status_code == 200:
            snapshots = response.json().get("archived_snapshots", {})
            if not snapshots:
                print("Page not archived, trying again")
                if tries < MAX_TRIES:
                    return archive_web_page(url, tries + 1)
                else:
                    raise RequestException(f"Max retries reached. Couldn't archive {url}")

        print("Web page archived successfully!")

    except (RequestException, HTTPError) as e:
        if tries < MAX_TRIES:
            print(f"Error: {e}. Retrying {tries+1}/{MAX_TRIES}")
            return archive_web_page(url, tries + 1)
        raise RequestException(f"Failed to archive {url} after {MAX_TRIES} attempts.")



if len(sys.argv) < 2:
    print("Usage: python archive.py <url>")
    sys.exit(1)

# Archive the web page
archive_web_page(sys.argv[1])
